{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHLl1X_T7YbC",
        "outputId": "e154c070-00de-43cb-8dd4-cd58abe3f891"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTusr7nD7aKN",
        "outputId": "3dbb901e-312a-4d04-ad2b-ed492adfca77"
      },
      "source": [
        "\n",
        "from google.colab import output\n",
        "!cp \"/content/drive/MyDrive/cnn_data.zip\" \"/content/cnn_data.zip\"\n",
        "# cnn_data.zip을 현재 디렉터리에 압축해제\n",
        "!unzip \"/content/cnn_data.zip\"\n",
        "!cp \"/content/drive/MyDrive/model.zip\" \"/content/model.zip\"\n",
        "!unzip \"/content/model.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/cnn_data.zip\n",
            "  inflating: dirty_mnist_2nd.zip     \n",
            "  inflating: dirty_mnist_2nd_answer.csv  \n",
            "  inflating: mnist_data.zip          \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_dirty_mnist_2nd.zip  \n",
            "Archive:  /content/model.zip\n",
            "  inflating: efficientnet-f0-23.pth  \n",
            "  inflating: efficientnet-f1-23.pth  \n",
            "  inflating: efficientnet-f2-23.pth  \n",
            "  inflating: efficientnet-f3-23.pth  \n",
            "  inflating: resnet101_32x8d-f0-12.pth  \n",
            "  inflating: resnet101_32x8d-f1-12.pth  \n",
            "  inflating: resnet101_32x8d-f2-12.pth  \n",
            "  inflating: resnet101_32x8d-f3-12.pth  \n",
            "  inflating: resnet101_32x8d-f4-12.pth  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBtKwc_iAaxJ"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/resnet101-f0-3.pth\" \"/content/resnet101-f0-3.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjFjpWUO7ehL"
      },
      "source": [
        "from google.colab import output\n",
        "!mkdir \"/content/dirty_mnist\"\n",
        "!unzip \"/content/dirty_mnist_2nd.zip\" -d \"/content/dirty_mnist/\"\n",
        "!mkdir \"/content/test_dirty_mnist\"\n",
        "!unzip \"/content/test_dirty_mnist_2nd.zip\" -d \"/content/test_dirty_mnist/\"\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znhq7ec4usJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ae7129-11bd-451d-8686-f928e417d80b"
      },
      "source": [
        "!pip install git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git\n",
            "  Cloning https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git to /tmp/pip-req-build-cp0a0222\n",
            "  Running command git clone -q https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git /tmp/pip-req-build-cp0a0222\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-poly-lr-decay==0.0.1) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: torch-poly-lr-decay\n",
            "  Building wheel for torch-poly-lr-decay (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-poly-lr-decay: filename=torch_poly_lr_decay-0.0.1-cp36-none-any.whl size=3832 sha256=72aa2edc9d3129a97694f5516b870b664ff9982a9cf2c00a76bd16e0fb1a8b0e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bwy34ogs/wheels/5a/b7/09/d748b20c9bdfc768a33c37a28b2ad7dd9ec3e79e5152cb1618\n",
            "Successfully built torch-poly-lr-decay\n",
            "Installing collected packages: torch-poly-lr-decay\n",
            "Successfully installed torch-poly-lr-decay-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITGmIHS7gkD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from typing import Tuple, Sequence, Callable\n",
        "import albumentations as A\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet101\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import csv\n",
        "from torchvision.models import resnet101\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "import torchvision.models as models\n",
        "from torch_poly_lr_decay import PolynomialLRDecay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz30Q0tItSPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c87311-2bd6-468e-8f7d-9209befd2e5d"
      },
      "source": [
        "def diagonal_reverse(img):\n",
        "    transformed_img = img.copy()\n",
        "    center = img.shape[0] // 2\n",
        "    transformed_img[0:center, 0:center] = img[center:center + center, center:center + center]\n",
        "    transformed_img[0:center, center:center + center] = img[center:center * 2, 0:center]\n",
        "    transformed_img[center:center + center, 0:center] = img[0:center, center:center * 2]\n",
        "    transformed_img[center:center + center, center:center + center] = img[0:center, 0:center]\n",
        "\n",
        "    return transformed_img\n",
        "\n",
        "\n",
        "dirty_mnist_answer = pd.read_csv(\"/content/dirty_mnist_2nd_answer.csv\")\n",
        "# %%\n",
        "\n",
        "dirty_mnist_answer.head()\n",
        "# %%\n",
        "\n",
        "dirty_mnist_answer[list(dirty_mnist_answer.columns.values)].describe()\n",
        "\n",
        "\n",
        "# %%\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "\n",
        "class config:\n",
        "    seed = 42\n",
        "    device = \"cuda:0\"\n",
        "\n",
        "    lr = 1e-4\n",
        "    epochs = 50\n",
        "    batch_size = 32\n",
        "    train_5_folds = True\n",
        "\n",
        "\n",
        "seed_everything(config.seed)\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "def split_dataset(path: os.PathLike) -> None:\n",
        "    df = pd.read_csv(path)\n",
        "    kfold = KFold(n_splits=5)\n",
        "    print(kfold)\n",
        "    for fold, (train, valid) in enumerate(kfold.split(df, df.index)):\n",
        "        df.loc[valid, 'kfold'] = int(fold)\n",
        "\n",
        "    df.to_csv('/content/split_kfold.csv', index=False)\n",
        "\n",
        "\n",
        "class DiagonalReverse(ImageOnlyTransform):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            always_apply=False,\n",
        "            p=1\n",
        "    ):\n",
        "        super(DiagonalReverse, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        return diagonal_reverse(img)\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dir: os.PathLike,\n",
        "            image_ids: os.PathLike,\n",
        "            transforms: Sequence[Callable],\n",
        "            albumentations: Sequence[Callable],\n",
        "    ) -> None:\n",
        "        self.dir = dir\n",
        "        self.transforms = transforms\n",
        "        self.albumentations = albumentations\n",
        "\n",
        "        self.labels = {}\n",
        "        with open(image_ids, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)\n",
        "            for row in reader:\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
        "\n",
        "        self.image_ids = list(self.labels.keys())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "        image_id = self.image_ids[index]\n",
        "        img = cv2.imread(\n",
        "            os.path.join(self.dir, f'{str(image_id).zfill(5)}.png'))\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
        "\n",
        "        if self.albumentations is not None:\n",
        "            transformed = self.albumentations(image=img)\n",
        "            transformed_image = transformed[\"image\"]\n",
        "            if self.transforms is not None:\n",
        "                image = self.transforms(transformed_image)\n",
        "        else:\n",
        "            if self.transforms is not None:\n",
        "                image = self.transforms(img)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "a_train = A.Compose([\n",
        "    A.HorizontalFlip(0.5),\n",
        "    A.RandomRotate90(0.5),\n",
        "    A.VerticalFlip(0.5),\n",
        "    A.OneOf([\n",
        "       A.GaussNoise(var_limit=[10, 50]),\n",
        "       A.MotionBlur(),\n",
        "       A.MedianBlur(),\n",
        "   ], p=0.2),\n",
        "   A.OneOf([\n",
        "       A.OpticalDistortion(distort_limit=1.0),\n",
        "       A.GridDistortion(num_steps=5, distort_limit=1.),\n",
        "       A.ElasticTransform(alpha=3),\n",
        "   ], p=0.2),\n",
        "    A.OneOf([\n",
        "      A.JpegCompression(),\n",
        "  ], p=0.2),\n",
        "    A.Cutout(num_holes=10, max_h_size=5, max_w_size=5, always_apply=False, p=0.5, ),\n",
        "    # DiagonalReverse(),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "class MnistModel(nn.Module):\n",
        "    def __init__(self, num_classes: int = 26) -> None:\n",
        "        super().__init__()\n",
        "        self.model_ft = models.resnext101_32x8d(pretrained=True)\n",
        "        self.classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(1000, 26)),\n",
        "                                                    \n",
        "                                                      ('output', nn.Sigmoid())\n",
        "                                                    ]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.model_ft(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "    \n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "         for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "# %%\n",
        "\n",
        "def train(fold: int, verbose: int = 100) -> None:\n",
        "    split_dataset('/content/dirty_mnist_2nd_answer.csv')\n",
        "    df = pd.read_csv('/content/split_kfold.csv')\n",
        "    df_train = df[df['kfold'] != fold].reset_index(drop=True)\n",
        "    df_valid = df[df['kfold'] == fold].reset_index(drop=True)\n",
        "\n",
        "    df_train.drop(['kfold'], axis=1).to_csv(f'/content/train-kfold-{fold}.csv', index=False)\n",
        "    df_valid.drop(['kfold'], axis=1).to_csv(f'/content/valid-kfold-{fold}.csv', index=False)\n",
        "\n",
        "    trainset = MnistDataset('/content/dirty_mnist', f'/content/train-kfold-{fold}.csv', transforms_train, a_train)\n",
        "    train_loader = DataLoader(trainset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    validset = MnistDataset('/content/dirty_mnist', f'/content/valid-kfold-{fold}.csv', transforms_test, None)\n",
        "    valid_loader = DataLoader(validset, batch_size=8, shuffle=False)\n",
        "\n",
        "    num_epochs = config.epochs\n",
        "    device = 'cuda'\n",
        "    \n",
        "\n",
        "    model = MnistModel().to(device)\n",
        "    model.load_state_dict(torch.load('/content/resnet101-f0-3.pth'))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    decay_steps = (len(trainset) // config.batch_size) * config.epochs\n",
        "    scheduler = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps,\n",
        "                                                           end_learning_rate=1e-6, power=0.9)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.9, momentum=0.9)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (i + 1) % verbose == 0:\n",
        "                outputs = outputs >= 0.5\n",
        "                acc = (outputs == targets).float().mean()\n",
        "                print(f'Fold {fold} | Epoch {epoch} | Train_L: {loss.item():.7f} | Train_A: {acc.item():.7f}')\n",
        "\n",
        "        model.eval()\n",
        "        valid_acc = 0.0\n",
        "        valid_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (images, targets) in enumerate(valid_loader):\n",
        "                images = images.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "                valid_loss += loss.item()\n",
        "                outputs = outputs >= 0.5\n",
        "                valid_acc += (outputs == targets).float().mean()\n",
        "            print(f'Fold {fold} | Epoch {epoch} | valid_L: {valid_loss / (i + 1):.7f} | valid_A: {valid_acc / (i + 1):.7f}\\n')\n",
        "            \n",
        "        torch.save(model.state_dict(), f'/content/resnet101-f{fold}-{epoch}.pth')\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train(0)\n",
        "    # train(1)\n",
        "    # train(2)\n",
        "    # train(3)\n",
        "    # train(4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KFold(n_splits=5, random_state=None, shuffle=False)\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2410287 | Train_A: 0.8954327\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2668787 | Train_A: 0.8822116\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2523417 | Train_A: 0.8870193\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2177269 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2646410 | Train_A: 0.8798077\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2221596 | Train_A: 0.8906251\n",
            "Fold 0 | Epoch 0 | Train_L: 0.3000396 | Train_A: 0.8725962\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2052894 | Train_A: 0.9218751\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2284116 | Train_A: 0.9038462\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2566340 | Train_A: 0.8918269\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2254525 | Train_A: 0.9098558\n",
            "Fold 0 | Epoch 0 | Train_L: 0.2315247 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 0 | valid_L: 0.2928199 | valid_A: 0.8944920\n",
            "\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2088315 | Train_A: 0.9062501\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2003022 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2163884 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 1 | Train_L: 0.1785472 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 1 | Train_L: 0.1400469 | Train_A: 0.9435096\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2622575 | Train_A: 0.8786058\n",
            "Fold 0 | Epoch 1 | Train_L: 0.1594783 | Train_A: 0.9375001\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2049430 | Train_A: 0.9086539\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2538756 | Train_A: 0.8798077\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2477342 | Train_A: 0.8834135\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2137189 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 1 | Train_L: 0.2795881 | Train_A: 0.8786058\n",
            "Fold 0 | Epoch 1 | valid_L: 0.2921008 | valid_A: 0.8939306\n",
            "\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1761160 | Train_A: 0.9302885\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2503755 | Train_A: 0.8942308\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2602530 | Train_A: 0.8906251\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1913227 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1743837 | Train_A: 0.9314904\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1993475 | Train_A: 0.9086539\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2248466 | Train_A: 0.9002404\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2724798 | Train_A: 0.8810096\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1967934 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2295085 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 2 | Train_L: 0.2350918 | Train_A: 0.8966346\n",
            "Fold 0 | Epoch 2 | Train_L: 0.1676955 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 2 | valid_L: 0.3243272 | valid_A: 0.8915578\n",
            "\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2359912 | Train_A: 0.9002404\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1724423 | Train_A: 0.9411058\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2570678 | Train_A: 0.8894231\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1973928 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2064736 | Train_A: 0.9002404\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2143203 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1467246 | Train_A: 0.9423077\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1945544 | Train_A: 0.9218751\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2181529 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1519962 | Train_A: 0.9387020\n",
            "Fold 0 | Epoch 3 | Train_L: 0.1864976 | Train_A: 0.9062501\n",
            "Fold 0 | Epoch 3 | Train_L: 0.2249263 | Train_A: 0.9086539\n",
            "Fold 0 | Epoch 3 | valid_L: 0.3005041 | valid_A: 0.8970810\n",
            "\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2806787 | Train_A: 0.8689904\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2682611 | Train_A: 0.8750001\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2142482 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2519360 | Train_A: 0.8858173\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2753121 | Train_A: 0.8798077\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2676311 | Train_A: 0.8737981\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2093330 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2781149 | Train_A: 0.8762019\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2421319 | Train_A: 0.8894231\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2255988 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 4 | Train_L: 0.2472794 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 4 | Train_L: 0.1921820 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 4 | valid_L: 0.2920871 | valid_A: 0.8971267\n",
            "\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2015279 | Train_A: 0.9110577\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2944156 | Train_A: 0.8677885\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2593694 | Train_A: 0.8822116\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2495810 | Train_A: 0.8966346\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2581812 | Train_A: 0.8990385\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2261603 | Train_A: 0.8942308\n",
            "Fold 0 | Epoch 5 | Train_L: 0.3068437 | Train_A: 0.8665866\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2534382 | Train_A: 0.8990385\n",
            "Fold 0 | Epoch 5 | Train_L: 0.1984205 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2010694 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2154552 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 5 | Train_L: 0.2548857 | Train_A: 0.8894231\n",
            "Fold 0 | Epoch 5 | valid_L: 0.2937145 | valid_A: 0.8972158\n",
            "\n",
            "Fold 0 | Epoch 6 | Train_L: 0.1642181 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 6 | Train_L: 0.1641636 | Train_A: 0.9242789\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2068272 | Train_A: 0.9086539\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2104318 | Train_A: 0.9170673\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2228842 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2124931 | Train_A: 0.9014423\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2713038 | Train_A: 0.8677885\n",
            "Fold 0 | Epoch 6 | Train_L: 0.1981822 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2168684 | Train_A: 0.9110577\n",
            "Fold 0 | Epoch 6 | Train_L: 0.1680119 | Train_A: 0.9362981\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2192996 | Train_A: 0.9002404\n",
            "Fold 0 | Epoch 6 | Train_L: 0.2252172 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 6 | valid_L: 0.2907613 | valid_A: 0.8995426\n",
            "\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1613703 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1582769 | Train_A: 0.9254808\n",
            "Fold 0 | Epoch 7 | Train_L: 0.2000547 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1832020 | Train_A: 0.9254808\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1296916 | Train_A: 0.9555289\n",
            "Fold 0 | Epoch 7 | Train_L: 0.2356273 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1849469 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 7 | Train_L: 0.2021235 | Train_A: 0.9026443\n",
            "Fold 0 | Epoch 7 | Train_L: 0.2398530 | Train_A: 0.8966346\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1215764 | Train_A: 0.9531251\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1963471 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 7 | Train_L: 0.1650617 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 7 | valid_L: 0.2945596 | valid_A: 0.8973418\n",
            "\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1823670 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 8 | Train_L: 0.2079906 | Train_A: 0.9062501\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1765256 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1699717 | Train_A: 0.9242789\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1913677 | Train_A: 0.9266827\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1622647 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1541040 | Train_A: 0.9338943\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1612753 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1619615 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 8 | Train_L: 0.2116162 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 8 | Train_L: 0.1688225 | Train_A: 0.9302885\n",
            "Fold 0 | Epoch 8 | Train_L: 0.2688538 | Train_A: 0.8701923\n",
            "Fold 0 | Epoch 8 | valid_L: 0.2866698 | valid_A: 0.9041545\n",
            "\n",
            "Fold 0 | Epoch 9 | Train_L: 0.2279026 | Train_A: 0.8954327\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1422464 | Train_A: 0.9387020\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1933559 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1950795 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 9 | Train_L: 0.2174769 | Train_A: 0.9182693\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1712524 | Train_A: 0.9302885\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1644357 | Train_A: 0.9423077\n",
            "Fold 0 | Epoch 9 | Train_L: 0.2444254 | Train_A: 0.9002404\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1348799 | Train_A: 0.9423077\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1777336 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1557681 | Train_A: 0.9350962\n",
            "Fold 0 | Epoch 9 | Train_L: 0.1830678 | Train_A: 0.9302885\n",
            "Fold 0 | Epoch 9 | valid_L: 0.2938467 | valid_A: 0.9028414\n",
            "\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1820309 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1829833 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1451864 | Train_A: 0.9387020\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1841012 | Train_A: 0.9158654\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1967091 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1895363 | Train_A: 0.9266827\n",
            "Fold 0 | Epoch 10 | Train_L: 0.2159561 | Train_A: 0.9086539\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1855092 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 10 | Train_L: 0.2225110 | Train_A: 0.8882212\n",
            "Fold 0 | Epoch 10 | Train_L: 0.2127428 | Train_A: 0.9050481\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1409331 | Train_A: 0.9411058\n",
            "Fold 0 | Epoch 10 | Train_L: 0.1718560 | Train_A: 0.9326923\n",
            "Fold 0 | Epoch 10 | valid_L: 0.3167614 | valid_A: 0.9016263\n",
            "\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1866997 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 11 | Train_L: 0.2262164 | Train_A: 0.8954327\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1850868 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1749973 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1586174 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 11 | Train_L: 0.2131425 | Train_A: 0.9134616\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1639810 | Train_A: 0.9375001\n",
            "Fold 0 | Epoch 11 | Train_L: 0.2321098 | Train_A: 0.8990385\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1718725 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1909730 | Train_A: 0.9062501\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1651269 | Train_A: 0.9242789\n",
            "Fold 0 | Epoch 11 | Train_L: 0.1651331 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 11 | valid_L: 0.3177095 | valid_A: 0.9014766\n",
            "\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1262572 | Train_A: 0.9495193\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1384126 | Train_A: 0.9375001\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1903960 | Train_A: 0.9158654\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1997017 | Train_A: 0.9098558\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1963658 | Train_A: 0.9170673\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1662998 | Train_A: 0.9314904\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1395521 | Train_A: 0.9338943\n",
            "Fold 0 | Epoch 12 | Train_L: 0.2064687 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1993472 | Train_A: 0.9182693\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1305162 | Train_A: 0.9507212\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1670503 | Train_A: 0.9326923\n",
            "Fold 0 | Epoch 12 | Train_L: 0.1898323 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 12 | valid_L: 0.3187282 | valid_A: 0.9031502\n",
            "\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1564177 | Train_A: 0.9338943\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1529070 | Train_A: 0.9242789\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1577214 | Train_A: 0.9278846\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1923622 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1920587 | Train_A: 0.9110577\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1816302 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1143772 | Train_A: 0.9507212\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1514670 | Train_A: 0.9350962\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1548714 | Train_A: 0.9314904\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1747135 | Train_A: 0.9170673\n",
            "Fold 0 | Epoch 13 | Train_L: 0.2444005 | Train_A: 0.8774039\n",
            "Fold 0 | Epoch 13 | Train_L: 0.1241129 | Train_A: 0.9507212\n",
            "Fold 0 | Epoch 13 | valid_L: 0.3296970 | valid_A: 0.9044076\n",
            "\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1046009 | Train_A: 0.9627404\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1120059 | Train_A: 0.9495193\n",
            "Fold 0 | Epoch 14 | Train_L: 0.2487847 | Train_A: 0.8918269\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1490628 | Train_A: 0.9314904\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1271651 | Train_A: 0.9423077\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1699181 | Train_A: 0.9266827\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1504429 | Train_A: 0.9350962\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1415524 | Train_A: 0.9338943\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1563089 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1293633 | Train_A: 0.9471154\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1450384 | Train_A: 0.9399039\n",
            "Fold 0 | Epoch 14 | Train_L: 0.1600835 | Train_A: 0.9218751\n",
            "Fold 0 | Epoch 14 | valid_L: 0.3255532 | valid_A: 0.9020738\n",
            "\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1897804 | Train_A: 0.9194712\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1945539 | Train_A: 0.9122596\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1751064 | Train_A: 0.9230770\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1646295 | Train_A: 0.9302885\n",
            "Fold 0 | Epoch 15 | Train_L: 0.2591198 | Train_A: 0.8737981\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1635104 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1402420 | Train_A: 0.9338943\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1967068 | Train_A: 0.9062501\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1602501 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1929538 | Train_A: 0.9146635\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1461237 | Train_A: 0.9326923\n",
            "Fold 0 | Epoch 15 | Train_L: 0.1715171 | Train_A: 0.9375001\n",
            "Fold 0 | Epoch 15 | valid_L: 0.3311965 | valid_A: 0.9044812\n",
            "\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1282780 | Train_A: 0.9447116\n",
            "Fold 0 | Epoch 16 | Train_L: 0.2483845 | Train_A: 0.8774039\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1704580 | Train_A: 0.9254808\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1193120 | Train_A: 0.9447116\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1775906 | Train_A: 0.9290866\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1346510 | Train_A: 0.9399039\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1806451 | Train_A: 0.9206731\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1391044 | Train_A: 0.9423077\n",
            "Fold 0 | Epoch 16 | Train_L: 0.0975078 | Train_A: 0.9615385\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1234534 | Train_A: 0.9447116\n",
            "Fold 0 | Epoch 16 | Train_L: 0.2127992 | Train_A: 0.9074519\n",
            "Fold 0 | Epoch 16 | Train_L: 0.1126527 | Train_A: 0.9471154\n",
            "Fold 0 | Epoch 16 | valid_L: 0.3390430 | valid_A: 0.9051145\n",
            "\n",
            "Fold 0 | Epoch 17 | Train_L: 0.1680485 | Train_A: 0.9134616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}